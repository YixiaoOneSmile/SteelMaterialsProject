{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate file: ./data/html_data/100Cr6/100Cr6_20241122_144548.html\n",
      "Removing duplicate file: ./data/html_data/100Cr6/100Cr6_20241122_144541.html\n",
      "Removing duplicate file: ./data/html_data/100Cr6/100Cr6_20241122_143508.html\n",
      "Removing duplicate file: ./data/html_data/100Cr6/100Cr6_20241122_143513.html\n",
      "Removing duplicate file: ./data/html_data/100Cr6/100Cr6_20241122_143503.html\n",
      "Removing duplicate file: ./data/html_data/X45NiCrMo4/X45NiCrMo 4_20241122_154011.html\n",
      "Removing duplicate file: ./data/html_data/10/10_20241204_173239.html\n",
      "Removing duplicate file: ./data/html_data/10/10_20241204_173236.html\n",
      "Removing duplicate file: ./data/html_data/10#/10#_20241121_213731.html\n",
      "Removing duplicate file: ./data/html_data/10#/10#_20241121_213727.html\n",
      "Removing duplicate file: ./data/html_data/M50/M50_20241122_152822.html\n",
      "Removing duplicate file: ./data/html_data/M2/M2_20241122_153941.html\n",
      "Removing duplicate file: ./data/html_data/20Cr/20Cr_20241125_155901.html\n",
      "Removing duplicate file: ./data/html_data/20Cr/20Cr_20241125_155906.html\n",
      "Removing duplicate file: ./data/html_data/20Cr/20Cr_20241125_155910.html\n",
      "Removing duplicate file: ./data/html_data/SAE1055/SAE1055_20241122_153218.html\n",
      "Removing duplicate file: ./data/html_data/SAE1055/SAE1055_20241122_153211.html\n"
     ]
    }
   ],
   "source": [
    "# 删除.data/html_data/重复的html文件\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    # 从文件名中提取时间戳，例如 \"100Cr6_20241122_143510.html\"\n",
    "    try:\n",
    "        date_str = filename.split('_')[1] + filename.split('_')[2].split('.')[0]\n",
    "        return datetime.strptime(date_str, '%Y%m%d%H%M%S')\n",
    "    except:\n",
    "        return datetime.min\n",
    "\n",
    "def find_and_remove_duplicates(directory):\n",
    "    title_map = {}  # {title_text: [(timestamp, file_path), ...]}\n",
    "    \n",
    "    # 首先收集所有文件信息\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    soup = BeautifulSoup(f, 'html.parser')\n",
    "                    title_tag = soup.find('title')\n",
    "                    \n",
    "                    if title_tag:\n",
    "                        title_text = title_tag.get_text(strip=True)\n",
    "                        timestamp = extract_timestamp(file)\n",
    "                        \n",
    "                        if title_text not in title_map:\n",
    "                            title_map[title_text] = []\n",
    "                        title_map[title_text].append((timestamp, file_path))\n",
    "\n",
    "    # 处理重复文件\n",
    "    for title_text, files in title_map.items():\n",
    "        if len(files) > 1:\n",
    "            # 按时间戳排序，保留最新的文件\n",
    "            sorted_files = sorted(files, key=lambda x: x[0], reverse=True)\n",
    "            # 保留第一个（最新的），删除其他的\n",
    "            for _, file_path in sorted_files[1:]:\n",
    "                print(f\"Removing duplicate file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = './data/html_data'\n",
    "    find_and_remove_duplicates(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr6_20241122_144548.html\n",
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr6_20241122_144541.html\n",
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr6_20241122_143508.html\n",
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr 6_20241122_144533.html\n",
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr6_20241122_143513.html\n",
      "删除重复文件: ./data/clean_html_data/100Cr6/100Cr6_20241122_143503.html\n",
      "删除重复文件: ./data/clean_html_data/X45NiCrMo4/X45NiCrMo 4_20241122_154011.html\n",
      "删除重复文件: ./data/clean_html_data/10#/10#_20241121_213731.html\n",
      "删除重复文件: ./data/clean_html_data/10#/10#_20241121_213727.html\n",
      "删除重复文件: ./data/clean_html_data/M50/M50_20241122_152822.html\n",
      "删除重复文件: ./data/clean_html_data/M2/M2_20241122_153941.html\n",
      "删除重复文件: ./data/clean_html_data/SAE1055/SAE1055_20241122_153218.html\n",
      "删除重复文件: ./data/clean_html_data/SAE1055/SAE1055_20241122_153211.html\n"
     ]
    }
   ],
   "source": [
    "# 清理clean_html_data文件夹中重复的html文件\n",
    "\n",
    "def clean_duplicate_html(directory):\n",
    "    title_map = {}  # {title_text: file_path}\n",
    "    \n",
    "    # 遍历所有html文件\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    soup = BeautifulSoup(f, 'html.parser')\n",
    "                    title_tag = soup.find('title')\n",
    "                    \n",
    "                    if title_tag:\n",
    "                        title_text = title_tag.get_text(strip=True)\n",
    "                        \n",
    "                        # 如果标题已存在,删除当前文件\n",
    "                        if title_text in title_map:\n",
    "                            print(f\"删除重复文件: {file_path}\")\n",
    "                            os.remove(file_path)\n",
    "                        else:\n",
    "                            # 保存第一个出现的文件\n",
    "                            title_map[title_text] = file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = './data/clean_html_data'\n",
    "    clean_duplicate_html(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除重复文件: ./data/JsonData/100Cr6/100Cr6_20241122_144530.json\n",
      "删除重复文件: ./data/JsonData/100Cr6/100Cr6_20241122_144538.json\n",
      "删除重复文件: ./data/JsonData/100Cr6/100Cr6_20241122_143510.json\n",
      "删除重复文件: ./data/JsonData/100Cr6/100Cr 6_20241122_143505.json\n",
      "删除重复文件: ./data/JsonData/100Cr6/100Cr6_20241122_143516.json\n",
      "删除重复文件: ./data/JsonData/100Cr6/100Cr6_20241122_144545.json\n",
      "删除重复文件: ./data/JsonData/X45NiCrMo4/X45NiCrMo 4_20241122_153615.json\n",
      "删除重复文件: ./data/JsonData/18CrMo4/18CrMo4_20241121_214420.json\n",
      "删除重复文件: ./data/JsonData/18CrMo4/18CrMo4_20241121_214341.json\n",
      "删除重复文件: ./data/JsonData/18CrMo4/18CrMo4_20241121_214415.json\n",
      "删除重复文件: ./data/JsonData/18CrMo4/18CrMo4_20241121_214408.json\n",
      "删除重复文件: ./data/JsonData/SCM420H/SCM420H_20241122_153256.json\n",
      "删除重复文件: ./data/JsonData/W6Mo5Cr4V2/W6Mo5Cr4V2_20241122_153447.json\n",
      "删除重复文件: ./data/JsonData/W6Mo5Cr4V2/W6Mo5Cr4V2_20241122_153442.json\n"
     ]
    }
   ],
   "source": [
    "# 删除.data/JsonData/重复的json文件\n",
    "import json\n",
    "def clean_duplicate_json(directory):\n",
    "    # 遍历所有材料文件夹\n",
    "    for root, dirs, _ in os.walk(directory):\n",
    "        for material_dir in dirs:\n",
    "            material_path = os.path.join(root, material_dir)\n",
    "            standard_map = {}  # {standard_code: (file_path, json_data)}\n",
    "            \n",
    "            # 遍历材料文件夹中的所有json文件\n",
    "            for file in os.listdir(material_path):\n",
    "                if file.endswith('.json'):\n",
    "                    file_path = os.path.join(material_path, file)\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        try:\n",
    "                            json_data = json.load(f)\n",
    "                            standard_code = json_data.get('Material', {}).get('BelongsToStandard', {}).get('StandardCode')\n",
    "                            \n",
    "                            if standard_code:\n",
    "                                if standard_code in standard_map:\n",
    "                                    # 发现重复的标准代码,删除当前文件\n",
    "                                    print(f\"删除重复文件: {file_path}\")\n",
    "                                    os.remove(file_path)\n",
    "                                else:\n",
    "                                    # 保存第一个出现的文件\n",
    "                                    standard_map[standard_code] = (file_path, json_data)\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"无法解析JSON文件: {file_path}\")\n",
    "                            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = './data/JsonData'\n",
    "    clean_duplicate_json(directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
